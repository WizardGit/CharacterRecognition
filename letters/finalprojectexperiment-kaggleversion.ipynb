{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Here we are importing the necessary libraries\nfrom __future__ import print_function\nfrom __future__ import division\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os\nimport copy\nimport torch.optim as optim\n#This is for Google Collab Notebook - it doesn't have torchplot installed!\n!pip install torchplot\nimport torchplot as tplt\n\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, models, transforms\nfrom torchvision import transforms as T\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, sampler, random_split\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"We will be using the:\", device)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:38:04.584960Z","iopub.execute_input":"2022-02-22T02:38:04.585420Z","iopub.status.idle":"2022-02-22T02:38:12.437036Z","shell.execute_reply.started":"2022-02-22T02:38:04.585379Z","shell.execute_reply":"2022-02-22T02:38:12.436300Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize(28),\n            transforms.CenterCrop(28),\n            transforms.ToTensor(),\n            #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\nemnist_dataset = datasets.EMNIST(root='./', split='letters', train=True, download=True, transform=transform,)\nall_data = datasets.ImageFolder('../input/handwritten-math-symbols/dataset', transform=transform)\n\nimage_datasets = torch.utils.data.ConcatDataset([all_data, emnist_dataset])\ntrain_data = DataLoader(all_data,batch_size=10,)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:38:12.439690Z","iopub.execute_input":"2022-02-22T02:38:12.439951Z","iopub.status.idle":"2022-02-22T02:38:12.589007Z","shell.execute_reply.started":"2022-02-22T02:38:12.439922Z","shell.execute_reply":"2022-02-22T02:38:12.588275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata = iter(train_data) # Let's iterate on it\nsingle_point = next(data)\nToPIL = T.ToPILImage() # Converting function\nimg0 = ToPIL(single_point[0][0])\nimg1 = ToPIL(single_point[0][1])\n# Plotting\nfig, axs = plt.subplots(1,2)\naxs[0].imshow(img0)\naxs[1].imshow(img1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:38:12.590366Z","iopub.execute_input":"2022-02-22T02:38:12.590917Z","iopub.status.idle":"2022-02-22T02:38:12.856337Z","shell.execute_reply.started":"2022-02-22T02:38:12.590878Z","shell.execute_reply":"2022-02-22T02:38:12.855652Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6,5,padding=2)\n        self.conv2 = nn.Conv2d(6,16,5, padding=0)\n        self.fc1 = nn.Linear(5*5*16, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84,62)\n        \n    def forward(self, x):\n        x = torch.sigmoid(self.conv1(x))\n        x =  nn.MaxPool2d(kernel_size=2, stride=2)(x)\n        x = torch.sigmoid(self.conv2(x))\n        x =  nn.MaxPool2d(kernel_size=2, stride=2)(x)\n        \n        x = x.view(-1, 5*5*16)\n        \n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-22T02:38:21.914943Z","iopub.execute_input":"2022-02-22T02:38:21.915775Z","iopub.status.idle":"2022-02-22T02:38:21.924680Z","shell.execute_reply.started":"2022-02-22T02:38:21.915718Z","shell.execute_reply":"2022-02-22T02:38:21.923825Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Let's set up some parameters\nlearning_rate=0.001\nnepochs = 10\nninputs=1*28*28\nnout=56\n\n#model = AlexNet().to(device)\nmodel = LeNet().to(device)\n\nprint(model)\n\n# We need an optimizer that tells us what form of gradient descent to do\noptimizer = torch.optim.Adam(model.parameters(), learning_rate)\n\n# We also need a loss function\nLossFunction = nn.CrossEntropyLoss()\n\nbatch_size = 64\n\ntransform = transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize(28),\n            transforms.CenterCrop(28),\n            transforms.ToTensor(),\n            #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n\n\nemnist_dataset = datasets.EMNIST(root='./', split='bymerge', train=True, download=True, transform=transform,)\nemnist_test_dataset = datasets.EMNIST(root='./', split='bymerge', train=False, download=False, transform=transform,)\noperator_dataset = datasets.ImageFolder('../input/handwritten-math-symbols/dataset', transform=transform)\n\ntrain_operator_data_len = int(len(operator_dataset)*0.70)\ntest_operator_data_len = int(len(operator_dataset) - train_operator_data_len)\noperator_train_data, operator_test_data = random_split(operator_dataset, [train_operator_data_len, test_operator_data_len])\n\ntrain_image_datasets = torch.utils.data.ConcatDataset([operator_train_data, emnist_dataset])\ntrain_data = DataLoader(train_image_datasets,batch_size=batch_size,shuffle=True)\n\ntest_image_datasets = torch.utils.data.ConcatDataset([operator_test_data, emnist_test_dataset])\ntest_data = DataLoader(test_image_datasets,batch_size=batch_size,shuffle=True)\n\n# This is default on but let's just be pedantic\nmodel.train()\nloss_history = []\ntest_accuracy_history = []\ntrain_accuracy_history = []\nloss = torch.Tensor([0])\nfor epoch in tqdm(range(nepochs), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n    accuracy = 0\n    for (data, label) in tqdm(train_data, desc=\"iteration\", unit=\"%\", disable=True):\n        # Here we clear the gradients\n        optimizer.zero_grad(set_to_none=True)\n        \n        # We need to make sure the tensors are on the same device as our model\n        data = data.to(device)\n        label = label.to(device)\n        out = model(data)\n        \n        loss = LossFunction(out, label)\n        \n        loss.backward() # This function calculates all our gradients\n        optimizer.step() # This function does our gradient descent with those gradients\n        loss_history.append(loss.item())\n        answers = out.max(dim=1)[1]\n        accuracy += (answers == label).sum()\n    # Append the training accuracy\n    accuracy = accuracy / len(train_image_datasets)*100\n    train_accuracy_history.append(accuracy)\n    \n    #let's get the test accuracy to see how well it generalizes\n    test_accuracy = 0\n    for (data, label) in test_data:\n        data = data.to(device)\n        label = label.to(device)\n        out = model(data)\n        answers = out.max(dim=1)[1]\n        test_accuracy += (answers == label).sum()\n    # Append the testing accuracy\n    test_accuracy = test_accuracy / len(test_image_datasets)*100\n    test_accuracy_history.append(test_accuracy)\n    #Print the results\n    print(f\"Epoch: {epoch} \\n Loss: {loss.item()} \\n Train Accuracy: {accuracy:.2f}% \\n Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:05:03.705143Z","iopub.execute_input":"2022-02-22T03:05:03.705559Z","iopub.status.idle":"2022-02-22T03:28:23.265603Z","shell.execute_reply.started":"2022-02-22T03:05:03.705515Z","shell.execute_reply":"2022-02-22T03:28:23.264892Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Plot the loss per iteration\nplt.plot(loss_history)\nplt.title(\"Neural Network Loss\")\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\n# Plot the accuracy per epoch\ntplt.plot(test_accuracy_history, 'r')\ntplt.plot(train_accuracy_history, 'b')\ntplt.title(\"Neural Network Accuracy\")\ntplt.xlabel(\"Epoch Number\")\ntplt.ylabel(\"Accuracy\")\ntplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:30:20.297967Z","iopub.execute_input":"2022-02-22T03:30:20.298595Z","iopub.status.idle":"2022-02-22T03:30:20.661800Z","shell.execute_reply.started":"2022-02-22T03:30:20.298556Z","shell.execute_reply":"2022-02-22T03:30:20.661132Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}