{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Here we are importing the necessary libraries\nfrom __future__ import print_function\nfrom __future__ import division\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os\nimport copy\nimport torch.optim as optim\n#This is for Google Collab Notebook - it doesn't have torchplot installed!\n#!pip install torchplot\n#import torchplot as tplt\n\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, models, transforms\nfrom torchvision import transforms as T\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, sampler, random_split\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"We will be using the:\", device)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:22:38.479645Z","iopub.execute_input":"2022-02-18T02:22:38.480125Z","iopub.status.idle":"2022-02-18T02:22:40.072546Z","shell.execute_reply.started":"2022-02-18T02:22:38.480089Z","shell.execute_reply":"2022-02-18T02:22:40.071849Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"emnist_dataset = datasets.EMNIST(root='./', # here\n                                split='letters',\n                                train=True, # train split\n                                download=True, # we want to get the data\n                                transform=T.ToTensor(), # put it into tensor format\n                              )\ntrain_data = DataLoader(emnist_dataset,\n                        batch_size=10,\n                        )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:22:43.011327Z","iopub.execute_input":"2022-02-18T02:22:43.012313Z","iopub.status.idle":"2022-02-18T02:23:11.418319Z","shell.execute_reply.started":"2022-02-18T02:22:43.012266Z","shell.execute_reply":"2022-02-18T02:23:11.417597Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata = iter(train_data) # Let's iterate on it\nsingle_point = next(data)\nToPIL = T.ToPILImage() # Converting function\nimg0 = ToPIL(single_point[0][0])\nimg1 = ToPIL(single_point[0][1])\n# Plotting\nfig, axs = plt.subplots(1,2)\naxs[0].imshow(img0)\naxs[1].imshow(img1)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:23:20.640132Z","iopub.execute_input":"2022-02-18T02:23:20.640737Z","iopub.status.idle":"2022-02-18T02:23:20.926394Z","shell.execute_reply.started":"2022-02-18T02:23:20.640680Z","shell.execute_reply":"2022-02-18T02:23:20.925782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6,5,padding=2)\n        self.conv2 = nn.Conv2d(6,16,5, padding=0)\n        self.fc1 = nn.Linear(5*5*16, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84,62)\n        \n    def forward(self, x):\n        x = torch.sigmoid(self.conv1(x))\n        x =  nn.MaxPool2d(kernel_size=2, stride=2)(x)\n        x = torch.sigmoid(self.conv2(x))\n        x =  nn.MaxPool2d(kernel_size=2, stride=2)(x)\n        \n        x = x.view(-1, 5*5*16)\n        \n        x = torch.sigmoid(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:23:25.947614Z","iopub.execute_input":"2022-02-18T02:23:25.948301Z","iopub.status.idle":"2022-02-18T02:23:25.957593Z","shell.execute_reply.started":"2022-02-18T02:23:25.948262Z","shell.execute_reply":"2022-02-18T02:23:25.956914Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#https://medium.com/analytics-vidhya/alexnet-a-simple-implementation-using-pytorch-30c14e8b6db2\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        #self.conv0 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 96, kernel_size= 11, stride=4, padding=0 )\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride= 1, padding= 2)\n        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride= 1, padding= 1)\n        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)\n        self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)\n        self.fc3 = nn.Linear(in_features=4096 , out_features=62)\n\n\n    def forward(self,x):\n        #x = F.relu(self.conv0(x))\n        x = F.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = self.maxpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:23:27.446464Z","iopub.execute_input":"2022-02-18T02:23:27.447087Z","iopub.status.idle":"2022-02-18T02:23:27.459574Z","shell.execute_reply.started":"2022-02-18T02:23:27.447049Z","shell.execute_reply":"2022-02-18T02:23:27.458920Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Let's set up some parameters\nlearning_rate=0.001\nnepochs = 10\nninputs=1*28*28\nnout=62\n\n#model = AlexNet().to(device)\nmodel = LeNet().to(device)\n\nprint(model)\n\n# We need an optimizer that tells us what form of gradient descent to do\noptimizer = torch.optim.Adam(model.parameters(), learning_rate)\n#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9,weight_decay=5e-4)\n\n# We also need a loss function\nLossFunction = nn.CrossEntropyLoss()\n\nbatch_size = 64\n\n#creating a dinstinct transform class for the train, validation and test dataset\ntranform_train = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3), \n    transforms.Resize((227,227)),     \n    transforms.ToTensor(), \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])])\n\nemnist_dataset = datasets.EMNIST(root='./', split='byclass', train=True, download=True, transform=T.ToTensor(),)\ntrain_data = DataLoader(emnist_dataset, batch_size=batch_size,)\n\nemnist_test_dataset = datasets.EMNIST(root='./', split='byclass', train=False, download=False, transform=T.ToTensor(),)\ntest_data = DataLoader(emnist_test_dataset, batch_size=batch_size,)\n\n\n# This is default on but let's just be pedantic\nmodel.train()\nloss_history = []\ntest_accuracy_history = []\ntrain_accuracy_history = []\nloss = torch.Tensor([0])\nfor epoch in tqdm(range(nepochs), desc=f\"Epoch\", unit=\"epoch\", disable=False):\n    accuracy = 0\n    for (data, label) in tqdm(train_data, desc=\"iteration\", unit=\"%\", disable=True):\n        # Here we clear the gradients\n        optimizer.zero_grad(set_to_none=True)\n        \n        # We need to make sure the tensors are on the same device as our model\n        data = data.to(device)\n        label = label.to(device)\n        out = model(data)\n        \n        loss = LossFunction(out, label)\n        \n        loss.backward() # This function calculates all our gradients\n        optimizer.step() # This function does our gradient descent with those gradients\n        loss_history.append(loss.item())\n        answers = out.max(dim=1)[1]\n        accuracy += (answers == label).sum()\n    # Append the training accuracy\n    accuracy = accuracy / len(emnist_dataset)*100\n    train_accuracy_history.append(accuracy)\n    \n    #let's get the test accuracy to see how well it generalizes\n    test_accuracy = 0\n    for (data, label) in test_data:\n        data = data.to(device)\n        label = label.to(device)\n        out = model(data)\n        answers = out.max(dim=1)[1]\n        test_accuracy += (answers == label).sum()\n    # Append the testing accuracy\n    test_accuracy = test_accuracy / len(emnist_test_dataset)*100\n    test_accuracy_history.append(test_accuracy)\n    #Print the results\n    print(f\"Epoch: {epoch} \\n Loss: {loss.item()} \\n Train Accuracy: {accuracy}% \\n Test Accuracy: {test_accuracy}%\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:36:52.390476Z","iopub.execute_input":"2022-02-18T02:36:52.390730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the loss per iteration\n    plt.plot(loss_history)\n    plt.title(\"Neural Network Loss\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n    # Plot the accuracy per epoch\n    tplt.plot(test_accuracy_history, 'r')\n    tplt.plot(train_accuracy_history, 'b')\n    tplt.title(\"Neural Network Accuracy\")\n    tplt.xlabel(\"Epoch Number\")\n    tplt.ylabel(\"Accuracy\")\n    tplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:25:04.078211Z","iopub.execute_input":"2022-02-18T01:25:04.079051Z","iopub.status.idle":"2022-02-18T01:25:14.544675Z","shell.execute_reply.started":"2022-02-18T01:25:04.079006Z","shell.execute_reply":"2022-02-18T01:25:14.543936Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Some sources - DON'T DELETE\n\n#https://www.nist.gov/itl/products-and-services/emnist-dataset#:~:text=The%20EMNIST%20Letters%20dataset%20merges,with%20the%20original%20MNIST%20dataset.\n#https://pytorch.org/vision/stable/datasets.html#emnist","metadata":{},"execution_count":null,"outputs":[]}]}